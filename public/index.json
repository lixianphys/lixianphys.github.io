[{"content":"Introduction MRI Technologies Magnetic Resonance Imaging (MRI) is a non-invasive imaging technique widely used in diagnosing various medical conditions, especially when it comes to examining soft tissues like the brain. MRI uses strong magnetic fields and radio waves to create detailed images of the body\u0026rsquo;s internal structures. This level of detail is crucial for identifying abnormalities, such as tumors, and has become a cornerstone in the diagnosis and monitoring of brain cancers.\nThree types of tumors (source: Cleveland Clinic website) A glioma is a tumor that forms when glial cells grow out of control. Normally, these cells support nerves and help your central nervous system work. Gliomas usually grow in the brain, but can also form in the spinal cord. Gliomas are malignant (cancerous), but some can be very slow growing. A meningioma is a tumor that forms in your meninges, which are the layers of tissue that cover your brain and spinal cord. They’re usually not cancerous (benign), but can sometimes be cancerous (malignant). Meningiomas are treatable. Pituitary adenomas are benign tumors on your pituitary gland. They’re noncancerous, but they can interfere with normal pituitary function and cause certain health conditions. Healthcare providers treat pituitary adenomas with surgery, medication, radiation or a combination of these therapies. How it started As I explore different applications of computer vision to improve work and life quality, I’ve noticed its significant growth in the field of medical imaging. Along the way, I discovered several high-quality MRI datasets on Kaggle, along with some inspiring notebooks (links provided at the end). I decided it would be a great opportunity to dive in and build my first end-to-end computer vision project— MRIMaster.\nThe goal of this project is to automatically classify MRI images of patients\u0026rsquo; brains into four categories: no tumor, glioma tumor, meningioma tumor, and pituitary tumor. This automation will enable doctors to focus on more critical tasks and make MRI services more affordable, benefiting both patients and the healthcare system\nChallenges The images from the Kaggle dataset are well-labeled and divided into four categories for training and evaluation. However, the challenge lies in the fact that tumors can vary in size, and each MRI image represents only a two-dimensional slice of the tumor and surrounding normal brain tissue. For this task, I chose a convolutional neural network (CNN) as the classifier, as it is widely used and excels in image classification problems.\nSolution Since this is an ongoing project, I will not only present the current solution but also propose future developments to enhance overall performance and facilitate the model\u0026rsquo;s deployment.\nStatus of Quo 1. Overview of the End-to-End Project This is the typical workflow including AWS service\rSince AWS EC2 GPU instances are quite expensive, I primarily work on my local machine and use Kaggle/Colab GPU resources to train and fine-tune models. Once the model choice is finalized, I plan to initialize cloud infrastructure reproducibly using Terraform and offload the heavy GPU tasks to an EC2 instance. To present prediction results, I’ve built a simple API service with FastAPI, allowing image uploads and returning predictions with corresponding confidence values. For cloud deployment, AWS SageMaker could be a more affordable solution.\n2. Metrics of training process Loss and accurancy during training process over epochs for training and evaluation sets\rAs observed, overfitting becomes evident after 30 epochs. Let\u0026rsquo;s take a closer look at the model\u0026rsquo;s performance across each category Confusion matrix for the evaluation set\rMore details (precision, recall and f1-score)\ncatergory precision recall f1-score glioma 0.88 0.92 0.90 meningioma 0.86 0.82 0.84 no tumor 0.86 0.85 0.85 pituitary 0.95 0.95 0.95 There is definitely room for improvement in this model to enhance its predictions.\n3. How to understand the model prediction To optimize the model (e.g.,hyperparameter tuning), we first need to understand where the model is performing well or poorly, and why. This is where Class Activation Mapping (CAM) becomes very useful, as it helps make convolutional neural networks more transparent to humans. Specifically, we use Grad-CAM to visualize the importance of each pixel in an image for any given prediction.\nThe model makes such obvious mistakes with very high confidence (over 0.95)\rHere are a few examples illustrating how poorly the model can perform on specific cases that should be relatively easy to identify as either tumors or healthy brains. Most concerning is the model\u0026rsquo;s high confidence when making these incorrect predictions. We don’t fully understand how the model arrives at such poor decisions, despite maintaining a decent overall success rate. This inconsistency is uncommon for human experts, who generally maintain steady performance and quality of work. To improve, we need to take a closer look at the model’s \u0026rsquo;thinking\u0026rsquo; process before we can truly claim to understand it.\nGrad-CAMs for each convolutional neural layer In order to obtain the class-discriminative localization map Grad-CAM $L_{Grad-CAM}^c$ of width $u$ and height $v$ for any class $c$, we first calculate the gradient of the score for class $c$, $y^c$, with respect to feature map (heat map) activations $A^k$ of a convolutional layer, i.e. $\\frac{\\partial y^c}{\\partial A^k}$. These gradients flowing back are global-average-pooled over the width and height dimensions (indexed by i and j respectively) to obtain the neuron importance weights $\\alpha_k^c$:\n$$ \\alpha_k^c = \\frac{1}{Z}\\sum_i{\\sum_j{\\frac{\\partial y^c}{\\partial A_{ij}^k}}} $$ We perform a weighted combination of forward activation maps, and follow it by a ReLU to obtain, $$ L_{Grad-CAM}^c = ReLU\\left(\\sum_k{\\alpha_k^cA^k}\\right) $$ From https://arxiv.org/abs/1610.02391\nThe first row of images represents the dot product of the activation maps $A^k$ and averaged gradients $\\alpha_k^c$ for each activation map in that layer after applyting ReLU. As we can observe, the size of each activation map decreases in the deeper layer due to MaxPool2d. The second row show the upsampled activation map. Now we aim to upsample $L_{Grad-CAM}^c$ after each convoluational layer to match the original size. This step is crucial for illustrating the importance of each pixel in the original image for class $c$. To enhance the visual effect, we overlay the original image with the upsampled activation map (heatmap).\nThe third row reveals, surprisingly, that the pixels associated with the tumor (white area at the top middle) are contributing the most to the incorrect prediction of \u0026rsquo;no tumor'.\n4. How to leverage existing model (Transfer learning) With a few lines of code, we can download the vgg16 model and easily adapt it to our classification task.\nfrom torchvision import models vgg16 = models.vgg16(pretrained=True,dropout=0.2) # Freeze the parameters in the feature extraction layers for param in vgg16.parameters(): param.requires_grad = False # Modify the classifier to match 4 output classes num_ftrs = vgg16.classifier[6].in_features vgg16.classifier[6] = nn.Linear(num_ftrs, 4) This pre-trained model has undergone 60 epochs of training, during which most parameters were frozen, with only 16,388 out of 134 million parameters being trainable.\nFrom the comparison of training process, we can highlight a few key points:\nOverfitting is effectively suppressed in the VGG16 model. The performance of the VGG16 model saturates early in the training phase but ultimately performs as well as the smaller CNN model after 60 epochs. This indicates that VGG16 requires less training to perform well, which is not surprising given its prior knowledge as a pre-trained model. To Do List [ ] Add object detection of the tumor (size) [ ] set up metrics, optimize the model via a variety of experiments listed in the preceeding section ⬛⬛⬛⬛⬜⬜⬜⬜⬜⬜(40% complete)\nTechnical Stack Development: PyTorch, Numpy, FastAPI, Unicorn, Pillow, Pandas, Matplotlib, Seaborn, Metaflow, openCV, sklearn, Git\nDeployment: AWS EC2, AWS S3, Docker, Terraform\nReference Kaggle dataset: https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri Kaggle notebook: https://www.kaggle.com/code/pkdarabi/brain-tumor-detection-by-cnn-pytorch Grad-CAM: https://arxiv.org/abs/1610.02391 ","permalink":"https://lixianphys.github.io/posts/mrimaster/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003ch3 id=\"mri-technologies\"\u003eMRI Technologies\u003c/h3\u003e\n\u003cp\u003eMagnetic Resonance Imaging (MRI) is a non-invasive imaging technique widely used in diagnosing various medical conditions, especially when it comes to examining soft tissues like the brain. MRI uses strong magnetic fields and radio waves to create detailed images of the body\u0026rsquo;s internal structures. This level of detail is crucial for identifying abnormalities, such as tumors, and has become a cornerstone in the diagnosis and monitoring of brain cancers.\u003c/p\u003e","title":"Brain Cancer Diagnosis by Artificial Intelligence"},{"content":"Imagine a scenario that a pair of flows move in opposite directions, and occasionally interact. We want to find out what these interactions lead to, given their initial states. There is no straightforward solution to this problem similar to in a co-propagating scenario. We will clarify the distinct nature of such counter-propagating flows and provide a complete solution to all possible interactions in a system of any size.\nWhat Does Interaction Mean? In an interaction, two flows are forced to exchange information about their current status (state), leading to immediate changes of status. After this interaction, the flows will move on with the updated status. When this interaction is strong (the strength is tunable from 0 to 1, resembling probability of status transition), the status of flows is expected to be significantly changed toward an average. Remarkably, this interaction is mathematically identical to a well-known stochastic process- Markov process.\nAn interaction can be represented by a transmission matrix $M(\\beta)$ in which the diagonal elements stand for the probability for remaining in original edge state and off-diagonal elements stand for exchange rate between edge states. This kind of matrix has two important properties: a. All the elements in a row add up to 1; b. All the elements in a column add up to 1. The dimension of this matrix is equal to the number of existing edge states.\n$$ \\mu_1^{\\prime}=\\left(1-\\frac{T_1T_2}{T_1+T_2}\\right)\\mu_1+\\left(\\frac{T_1T_2}{T_1+T_2}\\right)\\mu_2 $$\n$$ \\mu_2^{\\prime}=\\left(\\frac{T_1T_2}{T_1+T_2}\\right)\\mu_1+\\left(1-\\frac{T_1T_2}{T_1+T_2}\\right)\\mu_2 $$\n$$ \\begin{bmatrix}\\mu_1^{\\prime}\\ \\mu_2^{\\prime}\\end{bmatrix} =\\begin{bmatrix}1-\\beta/2\u0026amp;\\beta/2\\ \\beta/2\u0026amp;1-\\beta/2\\end{bmatrix} \\begin{bmatrix}\\mu_1\\ \\mu_2\\end{bmatrix}=M(\\beta)\\begin{bmatrix}\\mu_1\\ \\mu_2\\end{bmatrix} $$\n$$ \\beta=\\frac{2T_1T_2}{T_1+T_2}\\in[0,1] $$\nWhy Are Counter-propagating Flows So Different? A Sequence of Interactions For a single interaction, it is not necessary to distinguish co-propagating and counter-propagating mode. It is only when there are a sequence of interactions along the path, the distinct nature of counter-propagating edge states emerges. For co-propagating edge states, a sequence of interactions is relatively easy to take into account by repeatedly multiplying matrix representation of following interactions from head to end. This is because that the after-status coming out from a former interaction is exactly the before-status for the later interaction. However, this is not true for the counter-propagating situation where keeping track of the current status of each edge state becomes challenging and convoluted.\nHow to Model a Sequence of Interactions For co-propagating situation, it is not difficult to see that the combined effect of a sequence of interactions is a product of matrices representing each individual interaction. However, this straightforward approach is not applicable to the counter-propagating situation. We need to be careful in dealing this more complex situation and come up with an algorithm for a minimum system containing two edge states. Unfortunately, this algorithm is not applicable to larger systems (more flows) which involve different types of interactions. Simply speaking, this is due to the fact that the matrix representation of a combined effect should not be structurally similar to any individual interaction matrix. Therefore, we need a general model to deal with arbitrary number of flows and interaction, which will be covered in the next section.\nWhat Happens If More Flows and More Interactions Are Involved The complexity will increase rapidly when the number of flows and interactions increases. First, introducing more flows means more types of interactions. We simply can not find an analytical format for the combined matrix representation that is structurally identical to any individual matrix representation. In principle, we are still able to provide the analytic form for each matrix element. However, it is almost not writable and solvable by hand once there are more than a few interactions. For doing so, we need a general model that can be transformed into algorithms and implemented in programming language, like Python. To the best of my knowledge, such a general model at this level of complexity is still lacking.\nHow Can This Be Useful in Physics Interacting counter-propagating flows are physically realized in a quantum Hall system with edge states moving oppositely. Here I would like to introduce the physics background to illustrate how related research can benefit from this work.\nQuantum Hall effect and Its Edge-state Picture When a two-dimensional (extremely thin film), highly-conducting (high crystalline quality) semiconductor-like material ( e.g., GaAs heterostructure) is subject to a vertically aligned magnetic field (approx. 6 orders of magnitude stronger than Earth\u0026rsquo;s magnetic field), a profound and intriguing phenomenon, referred to as quantum Hall effect., appears and has been extensively studied since its birth in 1980. In history, several Noble Prizes have been awarded to related discoveries in 1985 , 1998 , 2016. Aside from its deeper physical implication like topology in physics , a well-celebrated, trustful \u0026ldquo;edge-state\u0026rdquo; physical picture for this phenomenon is that, electrons are forced to move alongside the thin film edges in one direction, forming dissipationless \u0026ldquo;edge states\u0026rdquo; (electron flows). Without delving into deeper theory, a \u0026ldquo;magic\u0026rdquo; force seems to guide the motion of electrons. Nevertheless, this simple picture is accurate enough to be applicable in academic discussions and exciting enough to spur expectations of revolution in the field of electric power transmission if no magnetic field is needed (See quantum spin Hall effect).\nLandauer-Büttiker Formalism and Interaction between Edge States Back to \u0026ldquo;edge states\u0026rdquo;, this unique \u0026ldquo;bullet-like\u0026rdquo; (ballistic) motion of electrons can be most suitably described by a model developed by Markus Büttiker in early 80s based on Rolf Landauer\u0026rsquo;s earlier work, later referred to as Landauer-Büttiker formalism . Often we do not assume these edge states interact with each other or exchange electrons. In a typical device with terminals (electron reservoirs), interactions between edge states are in principle not discernible if all edge states move in the same direction (a co-propagating mode). This is because that co-propagating edge states themselves are identical in terms of physical properties like electrochemical potential and any exchange between them would not lead to any observable consequence. In order to detect this type of interaction, dedicated device design is needed to divert co-propagating edge states into different paths. For counter-propagating edge states, this interaction is directly measurable even in a standard device (uniform path).\nCode Implementation The source code of this open-source project is currently host on Github\nDesign of Classes For python implementation, two classes are designed: System and Edge. In class Edge, a method trans_mat is built to implement the forward-propagation process and to return a matrix representation of combined effect of the interaction sequence represented by this Edge instance. Another method status_check is built to implement the forward- and back- propagation processes together to return all the status along the sequence. To initialize an instance of class system, we need to first initialize several Edge instances as input beforehand. This system class account for a real system that contains not only sequences of interactions but also terminals which separate edges. A method mastermat is designed to return the master matrix equation and another method solve will return its solution - a vector of terminal voltages.\nDynamic Programming for Diverting Edge States I would like to highlight the implementation of dynamic programming, which allows for diverting edge states into different paths. In a standard Landauer-Büttiker system, all edge states enter each terminal without exception. The electronic measurement is only performed via terminals which measure all edge states at once. Consequently, this measurement result is an average of all edge states in terms of electro-chemical potential.\nTo extract additional information of edge states, it is advisable to block some of them from entering particular terminals, leading to different paths. In other words, this additional controls imposed on edge states may reveal hidden information, not detectable in conventional configuration (standard Landauer-Büttiker system). However, a complex blockage pattern may also cause difficulty in defining the blocked edge state, because that the information carried by these edge states, not detectable by terminals, spread into other edge states via interactions directly or indirectly. An algorithm is developed to resolve this complexity recursively.\nAn algorithm is developed to resolve this complexity recursively.\nUsage The design principle for usage is to facilitate the bottom-up construction of a system from the building block - interaction. Hereby I would like to introduce how to build a system step-by-step with existing infrastructure from package counterfusion.\nA single interaction can be defined with a python snippet below:\nfrom counterfusion import interaction_builder left_stochastic_matrix = interaction_builder(dim=2,id1=0,id2=1,value=[0.1,0.3]) right_stochastic_matrix = left_stochastsic_matrix.T doubly_stochastic_matrix = interaction_builder(dim=2,id1=0,id2=1,value=0.5) An edge containing a sequence of interactions can be defined:\nfrom counterfusion import generate_bynumber, Edge #=============================================================== # General information for the edge - Hyperparameters totalNumMover = 4 numForwardMover = 2 initStates = [1,1,0.2,0.2] #=============================================================== # Information of scattering events # Interaction parameters v03 = 0.3 v01 = 0.5 v23 = 0.8 edgeDef = [[0,3,v03,10],[0,1,v01,10],[2,3,v23,10]] edgeInfo = generate_bynumber(edgeDef) edge = Edge(edgeInfo,totalNumMover,numForwardMover) A six-edge (terminal) system can be defined:\nfrom counterfusion import * # Define a six-terminal system # C1--M1--C2--M2--C3--M3--C4--M4--C5--M5--C6--M6--C1 # Total number of edge states: 4 # Number of forward-moving edge states: 2 (#0,#1) # Number of backward-moving edge states: 2 (#2,#3) #=============================================================== # General information for the system - Hyperparameters totalNumMover = 4 numForwardMover = 2 zeroVoltTerminal = 3 #=============================================================== # Information of scattering events # Interaction parameters v02 = 0.9 v13 = 0.7 v12 = 0.3 # Define interaction between nodes (contacts) # C1--M1--C2 edgeDef1 = [[0,2,v02,10],[1,3,v13,10]] # C2--M2--C3 edgeDef2 = [[0,2,v02,10],[1,2,v12,10]] # C3--M3--C4 edgeDef3 = [[0,2,v02,10],[1,3,v13,10]] # C4--M4--C5 edgeDef4 = [[0,2,v02,10],[1,2,v12,10]] # C5--M5--C6 edgeDef5 = [[0,2,v02,10],[1,3,v13,10]] # C6--M6--C1 edgeDef6 = [[0,2,v02,10],[1,2,v12,10]] #================================================================ edgesDef = [edgeDef1,edgeDef2,edgeDef3,edgeDef4,edgeDef5,edgeDef6] edgesInfo = [] for edgeDef in edgesDef: edgesInfo.append(generate_bynumber(edgeDef)) graph = [] for edgeInfo in edgesInfo: graph.append(Edge(edgeInfo,totalNumMover,numForwardMover)) nodesCurrent = [1,0,0,-1,0,0] sys = System(nodesCurrent,graph,numForwardMover,zeroVoltTerminal) Separate paths of edge states can be realized by editing blockStates:\n# The definition of blocking_state should strictly follow this rule: # [[index_of_terminal#1,[all blocked states in this terminal]],[[index_of_terminal#2,[all blocked states in this terminal],...]]] blockStates = [[1,[0]],[0,[2]],[2,[3]],[3,[1]]] sys = System(nodesCurrent,graph,numForwardMover,zeroVoltTerminal,blockStates) ","permalink":"https://lixianphys.github.io/posts/counterflow/","summary":"This blog serves as a memorial for the project titled to keep notes of why i started and what i learned from it","title":"A Duo Traveling Reversely"},{"content":"This blog serves as a memorial for the project titled \u0026ldquo;A genetic-algorithm optimized PID controller\u0026rdquo; to keep notes of why i started and what i learned from it.\nBackground and Motivation I came across an advert for Sous vide cooker, which is popular for preparing juicy steak. I happened to have an electronic kettle to be replaced. Not very happy with its condition, and a new one was already in my shopping list. Bingo! A natural idea came to me that this kettle can be repurposed to mimic what a Sous vide cooker could do - maintaining a fixed temperature of a body of water for a relatively long time. I knew immediately that this requires a good PID controller just like what I have in a cryostat for stabilizing the sample chamber temperature slightly above the absolute zero. As one may know, the brain of the PID controller is a simple feedback loop governed by three parameters: proportional gain (P), integral gain (I) and derivative gain (D). In practice, in a narrow range of goal temperature, these parameters do not need to be dynamically adapted to a different goal if the external environment is stable. However, it is not guaranteed to always achieve the best stabilization for a wide range of goal temperature for the same set of parameters. That means these parameters also need to be changed if the goal temperature varies. On top of the PID feedback loop, we also need to add another layer of feedback loop to optimize parameters P, I and D. Then the genetic algorithm seems to be useful here for generating the best choice of parameters. Here is a definition for genetic algorithm from MathWorks:\nA genetic algorithm (GA) is a method for solving both constrained and unconstrained optimization problems based on a natural selection process that mimics biological evolution. The algorithm repeatedly modifies a population of individual solutions. At each step, the genetic algorithm randomly selects individuals from the current population and uses them as parents to produce the children for the next generation. Over successive generations, the population \u0026ldquo;evolves\u0026rdquo; toward an optimal solution.\nThis experiment with genetic algorithm would at least guarantee me an enjoyable exploration if not a ready-to-use Sous vide cooker. Not too bad at the worst situation. Let\u0026rsquo;s go on the journey.\nHow to Implement This Idea How to Stablize the Temperature in General Before moving into the actual implementation, I would like to first compare a few ideas side-by-side for stablizing temperature, ranging from the most intuitive but also naive one to the less intuitive but more practical ones. I do not intend to specify programming language at this stage thus choose to illustrate in pseudo-code.\nIdea0 (without a PID controller): if measured (actual) temperature \u0026lt; the goal temperature: turn on the heater else: turn off the heater Idea1 (with a PID controller configured with a fixed set of parameters): Kp, Ki, Kd = x, x, x error = measured temperature - set temperature integral = error + integral derivative = (lasterror-error)/dt output = Kp*error+Ki*integral+Kd*derivative if output \u0026gt; 0: turn on the heater else: turn off the heater The PID controller will determine when to turn off and on the heater. However, we need to manually set the PID parameters Kp, Ki, Kd according to prior knowledge.\nIdea2 (Train the genetic algorithm in actual data): We use the genetic algorithm to predict the best parameters for a goal temperature. The genetic algorithm needs data to provide feedback in the closed loop of training. These data can be generated by traversing the PID parameter space in hundreds and thousands of experiments. Clearly, this approach is time-consuming and not practical if not involving automation process.\nIdea3 (Train the algorithm in a physical model): Thanks to the law of physics, we can first build up a physical model in which we can train the genetic algorithm in seconds. The implementation is then constituted by three steps: 1. Build a physical model as realistic as possible 2. Train the genetic algorithm in a physical model; 3. Deploy the optimized PID parameters to control hardware.\nPhysical Modeling Heat transfer can take place in four different ways : Advection, thermal conduction, convection and radiation. Here we only consider thermal conduction as the major contributor. We also consider metallic walls of the kettle to be perfectly transparent in terms of thermal conduction. Therefore, the hot water directly interfaces the cold air outside. Then we can start to do some calculations of relevant physical properties. According to Fourier\u0026rsquo;s law: heater transfer per $meter^2/sec$ = - thermal conductivity * gradient of temperature ~ 540 $Watt/meter^2$. The surface area of the kettle is estimated to be about 700 $cm^2$. Then the rate of heat dissipation is thus 37.8 W. The thermal conductivity of air is about 27 mW/m K. The heater power is 1000 Watt according to its manufacturer. Here is my sketch to illustrate the idea and some necessary math along the way: By knowing the heat capacity of water to be 4.184 Joule/gram/deg and 0.8L (800 gram) water inside, we can estimate that the temperature goes up at a rate of 0.3 deg/sec when the heater is on and decreases at a rate of 0.012 deg/sec when the heater is off. Note that this estimate is not a function of temperature. So far, we have finished the physical modeling part.\nTraining Process of the Genetic Algorithm In the training process, the simulated temperature is programmed to goes up by 0.32 degree in each heating cycle and decreases by 0.012 degree in each natural cooling cycle according to the results of physical modelling by assuming 1 cycle = 1 second.\nTo evaluate each set of PID parameters, a cost function is defined by L_mae= sum(abs(set temp - measured temp)) or L_mse=sum((set temp - measured temp)^2). At each generation, we select the set of parameters minimizing the cost function, and directly pass its properties onto the next generation (The principle of elitism) . Then we also provide a mechanism to mutate the \u0026ldquo;worst\u0026rdquo; set of parameters with the highest cost function value by pid_mutation, i.e., to fully randomize its values of Kp, Ki, Kd. For the rest, we would like to let them hybridize with neighbors to produce the next generation by pid_hybrid. This step helps to remain some good genetic properties the system, which are not fully manifested but potentially useful for later generations.\nThe code implementation is:\ndef pid_hybrid(pid1, pid2): dice = rand() * 3 if 0 \u0026lt;= dice \u0026lt; 1: pid1_new = np.array([pid1[0], pid2[1], pid2[2]]) pid2_new = np.array([pid2[0], pid1[1], pid1[2]]) elif 1 \u0026lt;= dice \u0026lt; 2: pid1_new = np.array([pid2[0], pid1[1], pid2[2]]) pid2_new = np.array([pid1[0], pid2[1], pid1[2]]) else: pid1_new = np.array([pid2[0], pid2[1], pid1[2]]) pid2_new = np.array([pid1[0], pid1[1], pid2[2]]) return pid1_new, pid2_new def pid_mutation(): return 100 * random_sample(3) Starting from the 99th generation (iteration), we find that all winners\u0026rsquo; descendants collapse in a smaller volume in (Kp, Ki, Kd) parameter space. That means the model has already been trained or optimized, thus ends the optimization process.\nFunctional parts Models Microcontroller Raspberry Pi Pico/ Ardurio Nano Temperature sensor MAX6675 220V control Relay User interface ICD 1602 User input rotary potentiometer Software The source code of this open-source project is hosted by Github\nAlgorithm Training Process For Python users, use pidtrain.py to train your PID controller in simulation. For C++ users, use pidtrain.cpp to train your PID controller instead. C++ code is 50 times faster than its Python counterpart. .\nDeploy to Microcontroller The Micropython code for microcontroller unit (MCU) is stored in mpython folder. Please include the libraries in lib as well. The Arduino code for MCU is stored in Arduino folder and please include the libraries folder\nUser Interface Design for the Microcontroller The LCD is only 16 by 2, so we have to fully exploit the limited screen space and the switch button of the rotary potentiometer.\nPerformance Test Results of Algorithm Training After training for 100 generations and a population size of 20 in each, a set of parameter (Kp = 88, Ki = 4.4, Kd = 18) is the final winner for a goal temperature of 56 degree Celsius.\nAlgorithm-optimized PID Controller and Its Performance We configure our PID controller by this set for following experiments. Here we show the actual temperature curve (labelled as real) and the simulation result (labelled as simulation). In the first 2000 seconds, the simulation matches quite well with the actual curve real in both trend of temperature (upper) and output (Kp*error+Ki*integral+Kd*derivative) of the PID system (lower). That means our physical model gives accurate predictions of temperature change rates used here. After 2000 seconds, the temperature in simulation stabilizes within 0.32 degree. In reality, the temperature continues to fluctuate in a much narrowed window ~ 5 degrees. This fluctuation seems to be overshooting, which may be due to the fact that the heater is still much hotter than water and continues to heat water even after being powered off. This may be the bottleneck to achieve better performance.\nConcluding remarks In the end, not surprisingly, there is still a lot to improve in many aspects for this combination of software and hardware to perform as good as a commercial cooker. For myself, it is thrilling to go through this exploration and broaden my knowledge in genetic algorithm and microcontrollers. I hope this is also some sort of entertaining for you to go with me in this journey.\n","permalink":"https://lixianphys.github.io/posts/pid/","summary":"This blog serves as a memorial for the project titled to keep notes of why i started and what i learned from it","title":"How to Turn My Kettle into a Sous Vide Cooker"},{"content":"This blog serves as a memorial for the project titled \u0026ldquo;A Landau level simulator\u0026rdquo; to keep notes of why i started and what i learned from it.\nBackground and motivation Landau level In solid-state physics, the famous band theory describes how electrons are distributed favorably over energy and momentum in electronic materials. The states (energy, momentum, spin and etc.) of electrons collectively form an electronic band structure. When subjecting to a magnetic field, these electronic states would shift in energy according to its interactive terms with the field. As this field gets very strong, the interactive term (orbital contribution) becomes dominant and start to form dense bands with narrow energy dispersion (A lot of states have almost the same energy). These bands are also energetically discrete and referred to as \u0026ldquo;Landau level\u0026rdquo;.\nExperimental Landau fans and different perspectives of theorists If scanning the Landau levels in energy (shift the observation window energetically in y-direction) for a series of increasing magnetic fields (x-direction), we would be able to revealing the energy information of various Landau levels and how they evolve with the field. However, in experimental measurements, this scan is not so straightforward to implement, because that we don\u0026rsquo;t control the energy of observation window (chemical potential) directly. The variation of energy is actually realized via controlling the population of electrons (electron density) as tuning the voltage applied to one side of a capacitor placed on top of a device (gate). Importantly, the highest energy of populated electrons (chemical potential) is not linearly dependent on the electron density. This difference (nonlinear transition) between theoretic picture and physical realization always causes disputes between theorists and experimentalists (Don\u0026rsquo;t ask why I know this ;P).\nAn experimental attempt: A Landau-level simulator To win our theorist friends and keep ourselves happy, we need to connect our experimental data (Landau levels evolves in electron density) with more favorable physical picture in theorist\u0026rsquo;s mind (Landau levels evolve in energy). A more challenging task is to interpret our experimental data and translate it into an electronic band structure. A simulator of Landau levels is thus aimed to provide theoretic prediction that is directly comparable to experiments for a predefined electronic band structure at zero magnetic field. If this prediction aligns with experiments well, we would be able to conclude the electronic band structure in an error-and-try manner. At worst, this Landau level simulator will hopefully provide some insights into making choice between multiple reasonable guesses.\nOverview The design of this project can be broken down into four steps: Firstly, construct an electronic band structure comprised of multiple single bands. These bands need to be defined by setting up parameters, e.g., choice of band (linear or quadratic), effective mass for quadratic bands, Fermi velocity for linear bands and zero-field energy point for each band. Secondly, we calculate the Landau level splitting as function of magnetic field. Thirdly, we translate this energy versus field into a density versus field plot. This process involves calculating the total density at each energy level (a loose definition of density of state), in another word, constructing an one-to-one correspondence between energy and density. The last step is to collect this transformed results of Landau levels at the chemical potential position in a density-versus-field plot for a series of discrete density values corresponding to experimentally attainable range. The collection of results will be then ready for a comparison with experiments (Landau fan chart).\nIntroduction to key aspects of a CLI software The source code of this open-source project is hosted in Github\nElectronic band structure The electronic band structure (energy versus momentum) of materials can be defined before entering actual calculations. For simplicity, it is assumed to be isotropic. Though this assumption seems to be oversimplified, we still can reach amazingly good match with experiments. This indicates that decorating the electron band by adding more terms and symmetries to its Hamiltonian is not as crucial as expected at least in the material system we studied.\nConfiguration Default settings such IO, plotting styles and some modelling parameters (Landau level broadening) are stored in config.py.\nFunctional descriptions Add a band to the system python addband.py [-density] {-is_dirac} {-is_cond} [-gfactor GFACTOR] [{-dp VF D} {-cp MEFF SPIN}] Remove band(s) from the system python delband.py [-i INDEX or all] Snapshot the system python peeksys.py Landau levels manifested in energy versus field python run.py [-enplot] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Landau levels manifested in density versus field python run.py [-denplot] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Simulation of Landau fan chart for given densities Plot the density versus field relationship for a set of given densities (specified by --allden and -nos) as a simulation of Landau fan chart. python run.py [-simu] [--allden \u0026#34;NS1 NE1 NS2 NE2 ...\u0026#34;] [-nos NOS] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Alternatively, one can load densities stored in a csv file (no header) under a directory DIR and with a filename FNM : python run.py [-simu] [-loadden path-to-csvfile] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Density of state (DOS) Plot the DOS versus field relationship at a fixed chemical potential python run.py [-dos] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Plot the DOS as function of density and field python run.py [-dosm] [-loadden path-to-csvfile] {-dir DIR} {-fnm FNM} [--enrange Estart Eend Enum] [--bfrange Bstart Bend Bnum] {-nmax NMAX} {-angle ANGLE} Plot the data from csv file python plotfile.py [-f path-to-csvfile] Introduction to modules module addband python addband.py [-option][parameters] Add a new band into the system. A system will be automatically initiated upon the creation of the first band.\nOption Description density density for this band in units of 1/m^2 -is_cond is a conduction band or not -gfactor g-factor for this band -dp DP1 DP2 For linearly dispersing bands. DP1: fermi velocity (in units of m/s) DP2: D (meV nm^2) for a Dirac-like band. D(kx^2+ky^2) accounts for the deviation from a linear Dirac dispersion. -cp CP1 CP2 For quadratically dispersing bands. CP1: absolute value of effective mass (in units of rest mass of electron) CP2: spin number (+1 or -1 or 0) for a conventional band. The spin number stands for spin-up (+1), spin-down (-1) and spinless (0), respectively module peeksys python peaksys.py Peek into the system you created. Once you\u0026rsquo;d like to see what is in your system, how many bands and their parameters. The above command will print out a summary of bands in table-like format.\nmodule delband python delband.py Remove bands from a system\nIt will prompt a dialogue which band to delete? Input the index number: Or press 'e' to exit. Type the number in front of all the parameters within a row for that band you want to delete. Or quit by typing e. In some cases, you need to delete bands in multiple-line scripts, then just using python toybands/delband.py -i [INDEX or all] \u0026lsquo;all\u0026rsquo; will remove all.\nmodule run python run.py [-option][parameters] Carry out various tasks in the existing material system.\nOption Description --enrange energy range: start end NumofPoints --bfrange magnetic field range: start end NumofPoints -enplot plot the energy versus bfield -denplot plot the density versus bfield -simu simulation -dosm mapping of DOS onto (n,B) -dos plot dos versus bfield --allden densities for each band: start1 end1 start2 end2 \u0026hellip; -nos number of steps in the simulation -dir relative output directory -fnm filename -nmax number of Landau levels to be considered (default=20) -angle angle in degree made with the sample plane norm by the external field (default=0) -scond set Landau level broadening parameter sigma at B = 1 T for conduction bands -sval set Landau level broadening parameter sigma at B = 1 T for valence bands -zll reduce the density of state (DOS) of zero LLs to its half compared to other LLs ","permalink":"https://lixianphys.github.io/posts/landaulevel-simulator/","summary":"This blog serves as a memorial for the project titled to keep notes of why i started and what i learned from it","title":"Reconciling Different Viewpoints of Landau Level Fan Chart"},{"content":"This blog serves as a memorial for the project titled \u0026ldquo;A versatile Python tool box for data analysis\u0026rdquo; to keep notes of why i started and what i learned from it.\nWorking in a laboratory, I spend a good amount of time on analyzing data acquired from various apparatus in the course of my daily routine. To drive the circle of workflow comprising experimental design, implementation and analysis efficiently, it is beneficial and advisable to get as much as information from data in limited time. It would be awful if one missed fantastic experimental ideas due to a lack of knowledge and understanding in existing data. In general, this requires practical thoughts and technique improvements in preparing data (pre-processing) and visualizing data (plotting). On one hand, I developed this tool box for my personal use and take it as an experimental attempt to improve the workflow for a laboratory researcher in a data-rich disciplinary, like physics and biology. On the other hand, I hope it, as an open-source project, can also encourage and inspire interested researchers to integrate a bit Python into their workflow for personalized purposes.\nDesign under the hood The source code of this open-source project is hosted at Github.\nStructure Its structure is simple that all scripts are placed under the root directory. The body of this project comprises three components: SciData.py, functions.py and other specially-purposed scripts like peakFind.py and startnb.py. SciData.py contains all the classes designed to prepare data existing in different types of raw data files. functions.py is a container for useful functions for simple calculation (e.g., fitting), data plotting and other specialized purposes(e.g., background remover, interpolation and differentiation) . Additionally, startnb.py provides a template for setting up a personalized Jupyter notebook environment; peakFind.py provides an algorithm for identifying peak positions in 1-dimensional data array.\nPrinciple The design principle can be broken down into three steps: Firstly, import and format raw data into a structured dataset defined by various classes; Secondly, methods of classes is utilized to handle pre-process, extraction of processed data and occasionally data visualization ; Lastly, a variety of functions are designed for later data processing and plotting. The second step is purposed to avoid potential pollution from any data processing after the initialization.\nUsage Caveat Though I intended to provide a general framework for data analysis in long-run, this project is so far still mainly personalized for my own use. Beware that the adaptation process requires some solid work. I hope this implementation and its design principle can be beneficial for anyone interested in building similar tools in diverse fields (Folk and PR are very welcome).\nTeam up with Jupyter notebook Personally, I like the interface of Jupyter notebook and the cell execution design very much. To maximize your gain from this project, it is highly recommended to use this package in a predefined Jupyter notebook environment. To do this, simply add the following snippet in front of your workspace in a notebook :\nfrom sys import platform if platform == \u0026#34;linux\u0026#34; or platform == \u0026#34;linux2\u0026#34;: %run ../../SciData/startnb.py # For a Linux machine else: %run ..\\..\\SciData\\startnb.py # For a Windows/Mac machine In startnb.py, one can add more frequently-used Python packages, change default plotting options and modify welcome messages to display the location and version of setting scripts.\nData preparation A good practice is to relocate all relevant raw data files into one place beforehand, e.g, under a directory pwd=r'../data' for more organized data handling. After raw data files being in place, we can simply choose a data structure (class) from the tool box. Here we use Datags as an example in single-line code data_structure_instance = Datags(attributes) . Till here, your raw data in a specified folder has been imported into a predefined container Datags. To get the processed data, simply call the method getdata() like your_structured_data = data_structure_instance.getdata(). Note that data structure DataX is designed to contain customized dataset with minimum limitations.\nPlotting Simple plot A plot option plotdata() is offered directly by the instance of data structure for one-shot quick visualization at the cost of more customized features. A more proper plot can be easily done with data columns extracted from the data body as demonstrated below:\nxcol,ycol = your_structured_data.xcol, your_structured_data.ycol plt.plot(xcol,ycol) 2D plot A method plotmap is equipped for a data structure Datamap intended for handling two-dimensional (2D) data. For other data structures, some tools are offered to construct a 2D array-like data, like diffz_df and fc_interp.\nInteractive plot In functions.py, several functions have been designed to produce interactive plots for various data structures, like plot_fc_analysis() and plot_fftmap().\nAn open discussion: how to improve your workflow In the course of my research career, I have used Origin, MATLAB and finally realized that Python and Jupyter notebook is my best choice for data analysis in recent years. I am going to reflect on how Python and Jupyter help me form my style in data analysis.\nRaw data management In data processing, we don\u0026rsquo;t want to mix up raw data with processed data. This requires good management of raw data. Firstly, all raw data will be located in a safe place to prevent from being accidentally modified. For each independent analysis, we directly import raw data\n","permalink":"https://lixianphys.github.io/posts/data-analysis/","summary":"This blog serves as a memorial for the project titled to keep notes of why i started and what i learned from it","title":"How Can Python Improve Work Flow in Data Analysis"}]